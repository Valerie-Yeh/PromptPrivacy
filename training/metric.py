import tensorflow as tf
import tensorflow_hub as hub
import pandas as pd
from tqdm import tqdm
import numpy as np
import evaluate
import re

class USE:
    def __init__(self):
        self.embed = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")
        print("Successfully fetched USE model.")

    def __call__(self, sentence1, sentence2):
        sentence1, sentence2 = sentence1.lower(), sentence2.lower()
        with tf.device('/CPU:0'):
            embeddings = self.embed([sentence1, sentence2])

        vector1 = tf.reshape(embeddings[0], [512, 1])
        vector2 = tf.reshape(embeddings[1], [512, 1])

        return tf.matmul(vector1, vector2, transpose_a=True).numpy()[0][0]

def compute_metrics(predictions, references):
    rouge = evaluate.load('rouge')
    bleu = evaluate.load('bleu')
    rouge_scores = rouge.compute(predictions=predictions, references=references)
    bleu_scores = bleu.compute(predictions=predictions, references=references)
    # Extract the median scores
    scores = {**rouge_scores, **bleu_scores}
    scores = {k: v * 100 for k, v in scores.items()}
    return {k: f'{scores[k]:.2f}' for k in ["rouge1", "rouge2", "rougeL", "bleu"]}

def normalize_pattern(pattern):
    # Remove spaces around special characters
    pattern = re.sub(r'\s*:\s*', ':', pattern)
    pattern = re.sub(r'\s*/\s*', '/', pattern)
    pattern = re.sub(r'\s*-\s*', '-', pattern)
    pattern = re.sub(r'\s*\.\s*', '.', pattern)
    pattern = re.sub(r'\s*@\s*', '@', pattern)
    return pattern

if __name__ == '__main__':

    df = pd.read_csv('../dataset_bi/predictions_560m.csv', encoding='utf-8')
    df_ref = pd.read_csv('../dataset_bi/test.csv', encoding='utf-8')

    use = USE()
    
    ### Evaluate predicted outputs and ground truths wich are directly generated by GPT4 ###
    
    use_score = []
    for i in tqdm(range(len(df['outputs']))):
        use_score.append(use(df['outputs'][i], df_ref['gt'][i]))

    use_score = np.array(use_score)
    scores = compute_metrics(df['outputs'].to_list(), df_ref['gt'].to_list())
    print("Trained Method Score:")
    print(f'USE mean = {use_score.mean():.2f}')
    print(scores)

    ### Replace labels in anonymized_outputs with given_words and then evaluate between ground truth ###
    replaced_anonymized_outputs = []
    
    for k in range(1000):
        tag = df_ref['given_words'][k]
        tag = tag[1:-1]
        tag = tag.split("', '")
        ent_split = [item.split(": ", 1)[1] for item in tag]
        label_split = [item.split(": ", 1)[0] for item in tag]
        ent_split = [normalize_pattern(item) for item in ent_split]

        dict = {}
        for j in range(0, len(ent_split)):
            t = label_split[j]
            dict[t] = ent_split[j]
        data = df_ref['anonymized_outputs'][k]
        #print(dict)
        for key in dict:
            data = data.replace(key, dict[key])
        
        replaced_anonymized_outputs.append(data)
    
    for k in range(1000, 2000):
        tag = df_ref['given_words'][k]
        tag = tag[2:-2]
        tag = tag.split("', '")
        ent_split = [item.split("': '", 1)[1] for item in tag]
        label_split = [item.split("': '", 1)[0] for item in tag]
        ent_split = [normalize_pattern(item) for item in ent_split]

        dict = {}
        for j in range(0, len(ent_split)):
            t = label_split[j]
            dict[t] = ent_split[j]
        data = df_ref['anonymized_outputs'][k]
        #print(dict)
        for key in dict:
            data = data.replace(key, dict[key])
        
        replaced_anonymized_outputs.append(data)
    
    replaced_use_score = []
    for s in range(len(df['outputs'])):
        replaced_use_score.append(use(replaced_anonymized_outputs[s], df_ref['gt'][s]))

    replaced_use_score = np.array(replaced_use_score)
    scores = compute_metrics(replaced_anonymized_outputs, df_ref['gt'].to_list())
    print("Replaced Method Score:")
    print(f'USE mean = {replaced_use_score.mean():.2f}')
    print(scores)
